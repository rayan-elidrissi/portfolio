\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{tcolorbox}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}

\colorlet{lsfree}{black}
\colorlet{lsexample}{green!65!black}
\colorlet{lstheorem}{blue!70!black}
\colorlet{lsproposition}{blue!70!black}
\colorlet{lslemma}{blue!70!black}
\colorlet{lsproblem}{red!80!black}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{problem}{Problem}[section]

\title{Linear Systems: Theory and Applications}
\author{MIT Mathematics Department}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Introduction to Linear Systems}

\subsection{Historical Context}

Linear systems have been studied since ancient times, with early developments in China (using the method of "Fangcheng" for solving systems of linear equations) and the Middle East. The modern theory of linear systems emerged with the development of matrix theory by Arthur Cayley and James Sylvester in the 19th century, and gained further momentum with the advent of computers in the 20th century.

\subsection{Definition and Basic Concepts}

\begin{definition}[Linear System]
A linear system is a collection of linear equations involving the same set of variables. A general form of a linear system with $n$ variables and $m$ equations is given by:
\begin{align}
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &= b_1 \\
a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n &= b_2 \\
\vdots \\
a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n &= b_m
\end{align}
where $a_{ij}$ are the coefficients, $x_j$ are the variables, and $b_i$ are the constant terms.
\end{definition}

\begin{definition}[Homogeneous Linear System]
A linear system is homogeneous if all the constant terms $b_i$ are zero:
\begin{align}
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &= 0 \\
a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n &= 0 \\
\vdots \\
a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n &= 0
\end{align}
\end{definition}

\begin{remark}
A homogeneous system always has at least one solution, namely, the trivial solution where all variables are zero: $x_1 = x_2 = \cdots = x_n = 0$.
\end{remark}

\section{Matrix Representation of Linear Systems}

\subsection{Coefficient Matrix and Augmented Matrix}

\begin{definition}[Coefficient Matrix]
For a linear system of $m$ equations with $n$ variables, the coefficient matrix $A$ is the $m \times n$ matrix where each entry $a_{ij}$ represents the coefficient of variable $x_j$ in equation $i$:
\begin{equation}
A = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{pmatrix}
\end{equation}
\end{definition}

\begin{definition}[Augmented Matrix]
The augmented matrix of a linear system is formed by appending the column of constant terms to the coefficient matrix:
\begin{equation}
[A|b] = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} & | & b_1 \\
a_{21} & a_{22} & \cdots & a_{2n} & | & b_2 \\
\vdots & \vdots & \ddots & \vdots & | & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} & | & b_m
\end{pmatrix}
\end{equation}
\end{definition}

\begin{definition}[Matrix Equation Form]
A linear system can be written in matrix form as $Ax = b$, where:
\begin{equation}
A = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{pmatrix}, \quad
x = \begin{pmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{pmatrix}, \quad
b = \begin{pmatrix}
b_1 \\
b_2 \\
\vdots \\
b_m
\end{pmatrix}
\end{equation}
\end{definition}

\subsection{Elementary Row Operations}

\begin{definition}[Elementary Row Operations]
The following operations on the rows of a matrix are called elementary row operations:
\begin{enumerate}[label=(\roman*)]
\item Interchange two rows: $R_i \leftrightarrow R_j$
\item Multiply a row by a non-zero scalar: $R_i \rightarrow kR_i$ $(k \neq 0)$
\item Add a multiple of one row to another: $R_i \rightarrow R_i + kR_j$ $(i \neq j)$
\end{enumerate}
\end{definition}

\begin{theorem}[Invariance of Solutions]
Elementary row operations on the augmented matrix of a linear system do not change the solution set of the system.
\end{theorem}

\section{Gaussian Elimination and Echelon Forms}

\subsection{Row Echelon Form}

\begin{definition}[Row Echelon Form]
A matrix is in row echelon form (REF) if:
\begin{enumerate}[label=(\roman*)]
\item All rows consisting entirely of zeros, if any, are at the bottom of the matrix.
\item The leading entry (first non-zero element) in each non-zero row is to the right of the leading entry in the row above it.
\item All entries in a column below a leading entry are zeros.
\end{enumerate}
\end{definition}

\begin{definition}[Reduced Row Echelon Form]
A matrix is in reduced row echelon form (RREF) if:
\begin{enumerate}[label=(\roman*)]
\item It is in row echelon form.
\item Each leading entry is 1.
\item All entries in a column above a leading 1 are zeros.
\end{enumerate}
\end{definition}

\begin{theorem}[Existence and Uniqueness of RREF]
Every matrix can be transformed into a unique reduced row echelon form using elementary row operations.
\end{theorem}

\begin{example}[Row Echelon Form]
The following matrix is in row echelon form:
\begin{equation}
\begin{pmatrix}
1 & 3 & 0 & 2 \\
0 & 0 & 2 & -1 \\
0 & 0 & 0 & 5
\end{pmatrix}
\end{equation}
The leading entries are at positions $(1,1)$, $(2,3)$, and $(3,4)$.
\end{example}

\begin{example}[Reduced Row Echelon Form]
The reduced row echelon form of the above matrix is:
\begin{equation}
\begin{pmatrix}
1 & 3 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{pmatrix}
\end{equation}
\end{example}

\subsection{Gaussian Elimination Algorithm}

\begin{theorem}[Gaussian Elimination]
Any augmented matrix can be transformed into row echelon form using the following steps:
\begin{enumerate}[label=(\arabic*)]
\item Start with the leftmost column and find the topmost non-zero entry. If all entries are zero, move to the next column.
\item Swap rows if necessary to bring this non-zero entry to the top.
\item Scale this row to make the leading entry 1.
\item Eliminate all entries below the leading 1 by adding appropriate multiples of the current row.
\item Cover the current row and repeat the process on the submatrix below.
\end{enumerate}
\end{theorem}

\begin{theorem}[Gauss-Jordan Elimination]
To obtain the reduced row echelon form, after performing Gaussian elimination, continue as follows:
\begin{enumerate}[label=(\arabic*)]
\item Starting from the rightmost leading 1, eliminate all non-zero entries above it by adding appropriate multiples of the current row.
\item Move to the left to the next leading 1 and repeat.
\end{enumerate}
\end{theorem}

\section{Solution of Linear Systems}

\subsection{Types of Linear Systems}

\begin{definition}[Consistent and Inconsistent Systems]
A linear system is:
\begin{enumerate}[label=(\roman*)]
\item Consistent if it has at least one solution.
\item Inconsistent if it has no solution.
\end{enumerate}
\end{definition}

\begin{theorem}[Characterization of Consistent Systems]
A linear system $Ax = b$ is consistent if and only if the rank of the coefficient matrix equals the rank of the augmented matrix, i.e., $\text{rank}(A) = \text{rank}([A|b])$.
\end{theorem}

\begin{definition}[Uniqueness of Solutions]
A consistent linear system has:
\begin{enumerate}[label=(\roman*)]
\item A unique solution if the number of pivot columns in the coefficient matrix equals the number of variables.
\item Infinitely many solutions if the number of pivot columns in the coefficient matrix is less than the number of variables.
\end{enumerate}
\end{definition}

\begin{theorem}[Relation to Rank]
For a linear system $Ax = b$ with $n$ variables:
\begin{enumerate}[label=(\roman*)]
\item If $\text{rank}(A) = \text{rank}([A|b]) = n$, then the system has a unique solution.
\item If $\text{rank}(A) = \text{rank}([A|b]) < n$, then the system has infinitely many solutions.
\item If $\text{rank}(A) < \text{rank}([A|b])$, then the system is inconsistent.
\end{enumerate}
\end{theorem}

\subsection{General and Particular Solutions}

\begin{definition}[General Solution]
The general solution of a consistent linear system expresses every solution in terms of:
\begin{enumerate}[label=(\roman*)]
\item A particular solution of the non-homogeneous system $Ax = b$.
\item The general solution of the associated homogeneous system $Ax = 0$.
\end{enumerate}
\end{definition}

\begin{theorem}[Structure of Solutions]
If $x_p$ is a particular solution of $Ax = b$ and $x_h$ is the general solution of $Ax = 0$, then the general solution of $Ax = b$ is $x = x_p + x_h$.
\end{theorem}

\begin{example}[General Solution]
Consider the system:
\begin{align}
x_1 + 2x_2 - x_3 &= 5 \\
2x_1 + 4x_2 + x_3 &= 8
\end{align}

The augmented matrix and its row echelon form are:
\begin{equation}
\begin{pmatrix}
1 & 2 & -1 & | & 5 \\
2 & 4 & 1 & | & 8
\end{pmatrix} \sim
\begin{pmatrix}
1 & 2 & -1 & | & 5 \\
0 & 0 & 3 & | & -2
\end{pmatrix} \sim
\begin{pmatrix}
1 & 2 & 0 & | & \frac{11}{3} \\
0 & 0 & 1 & | & -\frac{2}{3}
\end{pmatrix}
\end{equation}

The system has infinitely many solutions with $x_2$ as a free variable. If we set $x_2 = t$, we get:
\begin{align}
x_1 &= \frac{11}{3} - 2t \\
x_3 &= -\frac{2}{3}
\end{align}

This gives the general solution as $x = (x_1, x_2, x_3) = (\frac{11}{3} - 2t, t, -\frac{2}{3})$ for any $t \in \mathbb{R}$.
\end{example}

\section{Vector Spaces and Linear Systems}

\subsection{Column Space and Row Space}

\begin{definition}[Column Space]
The column space of a matrix $A$, denoted by $\text{Col}(A)$, is the span of the columns of $A$:
\begin{equation}
\text{Col}(A) = \text{span}\{a_1, a_2, \ldots, a_n\}
\end{equation}
where $a_j$ is the $j$-th column of $A$.
\end{definition}

\begin{definition}[Row Space]
The row space of a matrix $A$, denoted by $\text{Row}(A)$, is the span of the rows of $A$:
\begin{equation}
\text{Row}(A) = \text{span}\{r_1, r_2, \ldots, r_m\}
\end{equation}
where $r_i$ is the $i$-th row of $A$.
\end{definition}

\begin{theorem}[Dimension of Column and Row Spaces]
For any matrix $A$:
\begin{equation}
\dim(\text{Col}(A)) = \dim(\text{Row}(A)) = \text{rank}(A)
\end{equation}
\end{theorem}

\begin{theorem}[Relationship to Linear Systems]
For a linear system $Ax = b$:
\begin{enumerate}[label=(\roman*)]
\item The system is consistent if and only if $b \in \text{Col}(A)$.
\item The number of free variables in the solution equals $n - \text{rank}(A)$.
\end{enumerate}
\end{theorem}

\subsection{Null Space}

\begin{definition}[Null Space]
The null space of a matrix $A$, denoted by $\text{Null}(A)$, is the set of all vectors $x$ such that $Ax = 0$:
\begin{equation}
\text{Null}(A) = \{x \in \mathbb{R}^n : Ax = 0\}
\end{equation}
\end{definition}

\begin{theorem}[Dimension of Null Space]
The dimension of the null space of an $m \times n$ matrix $A$ is:
\begin{equation}
\dim(\text{Null}(A)) = n - \text{rank}(A)
\end{equation}
This is also called the nullity of $A$.
\end{theorem}

\begin{theorem}[Rank-Nullity Theorem]
For an $m \times n$ matrix $A$:
\begin{equation}
\text{rank}(A) + \dim(\text{Null}(A)) = n
\end{equation}
\end{theorem}

\begin{theorem}[Relationship to Homogeneous Systems]
For a homogeneous system $Ax = 0$:
\begin{enumerate}[label=(\roman*)]
\item The system always has the trivial solution $x = 0$.
\item The system has non-trivial solutions if and only if $\text{rank}(A) < n$.
\item The set of all solutions is precisely $\text{Null}(A)$.
\end{enumerate}
\end{theorem}

\section{Applications of Linear Systems}

\subsection{Network Flow Problems}

\begin{definition}[Network Flow]
A network flow problem involves finding the optimal flow of resources through a network, subject to capacity constraints and conservation of flow at each node.
\end{definition}

\begin{example}[Kirchhoff's Laws in Electrical Circuits]
For an electrical circuit with $n$ nodes and $m$ branches:
\begin{enumerate}[label=(\roman*)]
\item Kirchhoff's Current Law (KCL): The sum of currents entering a node equals the sum of currents leaving the node. This gives $n-1$ independent equations.
\item Kirchhoff's Voltage Law (KVL): The sum of voltage drops around any closed loop is zero. This gives $m-n+1$ independent equations.
\end{enumerate}
Together, these laws form a linear system with $m$ variables (branch currents) and $m$ equations.
\end{example}

\subsection{Economic Models}

\begin{example}[Leontief Input-Output Model]
In the Leontief model, an economy with $n$ sectors can be described by:
\begin{equation}
x = Ax + d
\end{equation}
where $x$ is the vector of total outputs, $A$ is the matrix of technological coefficients (with $a_{ij}$ representing the amount of sector $i$'s output needed to produce one unit of sector $j$'s output), and $d$ is the vector of final demands.

Solving for $x$:
\begin{equation}
x = (I - A)^{-1} d
\end{equation}
\end{example}

\subsection{Computer Graphics and Transformations}

\begin{example}[3D Transformations]
In computer graphics, transformations such as translation, rotation, and scaling can be represented as matrices. For a point $(x, y, z)$ in 3D space, these transformations are applied by solving the linear system:
\begin{equation}
\begin{pmatrix}
x' \\
y' \\
z' \\
1
\end{pmatrix} = 
\begin{pmatrix}
a_{11} & a_{12} & a_{13} & a_{14} \\
a_{21} & a_{22} & a_{23} & a_{24} \\
a_{31} & a_{32} & a_{33} & a_{34} \\
0 & 0 & 0 & 1
\end{pmatrix}
\begin{pmatrix}
x \\
y \\
z \\
1
\end{pmatrix}
\end{equation}
where $(x', y', z')$ is the transformed point.
\end{example}

\section{Linear Differential Equations}

\subsection{Systems of Linear Differential Equations}

\begin{definition}[Linear System of ODEs]
A system of first-order linear differential equations can be written as:
\begin{equation}
\frac{d\mathbf{x}}{dt} = A(t)\mathbf{x} + \mathbf{b}(t)
\end{equation}
where $\mathbf{x}(t)$ is the vector of unknown functions, $A(t)$ is the coefficient matrix, and $\mathbf{b}(t)$ is the vector of non-homogeneous terms.
\end{definition}

\begin{theorem}[General Solution of Homogeneous System]
For a homogeneous system $\frac{d\mathbf{x}}{dt} = A\mathbf{x}$ with constant coefficients, the general solution is:
\begin{equation}
\mathbf{x}(t) = e^{At}\mathbf{c}
\end{equation}
where $\mathbf{c}$ is a constant vector.
\end{theorem}

\begin{theorem}[Variation of Parameters]
For a non-homogeneous system $\frac{d\mathbf{x}}{dt} = A\mathbf{x} + \mathbf{b}(t)$ with constant coefficients, if $\Phi(t)$ is a fundamental matrix solution of the homogeneous system, then the general solution is:
\begin{equation}
\mathbf{x}(t) = \Phi(t)\mathbf{c} + \Phi(t)\int \Phi^{-1}(s)\mathbf{b}(s) \, ds
\end{equation}
\end{theorem}

\section{Numerical Methods for Linear Systems}

\subsection{Direct Methods}

\begin{theorem}[LU Decomposition]
If $A$ is a square matrix with non-zero principal minors, then $A$ can be factorized as $A = LU$, where $L$ is a lower triangular matrix with 1's on the diagonal and $U$ is an upper triangular matrix.
\end{theorem}

\begin{theorem}[Computational Complexity]
The computational complexity of solving a linear system using Gaussian elimination is $O(n^3)$ for an $n \times n$ matrix.
\end{theorem}

\subsection{Iterative Methods}

\begin{definition}[Jacobi Method]
For a linear system $Ax = b$, where $A = D + L + U$ with $D$ diagonal, $L$ strictly lower triangular, and $U$ strictly upper triangular, the Jacobi iteration is:
\begin{equation}
x^{(k+1)} = D^{-1}(b - (L + U)x^{(k)})
\end{equation}
\end{definition}

\begin{definition}[Gauss-Seidel Method]
Under the same conditions, the Gauss-Seidel iteration is:
\begin{equation}
x^{(k+1)} = (D + L)^{-1}(b - Ux^{(k)})
\end{equation}
\end{definition}

\begin{theorem}[Convergence of Iterative Methods]
Both Jacobi and Gauss-Seidel methods converge for any initial guess if $A$ is strictly diagonally dominant, i.e., $|a_{ii}| > \sum_{j \neq i} |a_{ij}|$ for all $i$.
\end{theorem}

\section{Advanced Topics}

\subsection{Sensitivity and Condition Number}

\begin{definition}[Condition Number]
The condition number of a non-singular matrix $A$ is:
\begin{equation}
\kappa(A) = \|A\| \cdot \|A^{-1}\|
\end{equation}
where $\|\cdot\|$ is a matrix norm.
\end{definition}

\begin{theorem}[Sensitivity of Linear Systems]
For a perturbed linear system $(A + \delta A)(x + \delta x) = b + \delta b$, the relative error in the solution satisfies:
\begin{equation}
\frac{\|\delta x\|}{\|x\|} \leq \kappa(A) \left( \frac{\|\delta A\|}{\|A\|} + \frac{\|\delta b\|}{\|b\|} \right) + O(\|\delta A\|^2, \|\delta b\|^2)
\end{equation}
\end{theorem}

\subsection{Generalized Eigenvalue Problems}

\begin{definition}[Generalized Eigenvalue Problem]
Given matrices $A$ and $B$, find scalar $\lambda$ and non-zero vector $x$ such that:
\begin{equation}
Ax = \lambda Bx
\end{equation}
\end{definition}

\begin{theorem}[Solution via Linear Systems]
The generalized eigenvalue problem can be solved by considering the family of linear systems $(A - \lambda B)x = 0$ for various values of $\lambda$.
\end{theorem}

\section{Advanced Exercises}

\begin{problem}
Prove that if $A$ is an $n \times n$ matrix, then $\text{rank}(A^k) \leq \text{rank}(A^{k-1})$ for any positive integer $k$.
\end{problem}

\begin{problem}
Show that if $A$ and $B$ are similar matrices (i.e., there exists an invertible matrix $P$ such that $B = P^{-1}AP$), then they have the same rank, determinant, and characteristic polynomial.
\end{problem}

\begin{problem}
For the linear system $Ax = b$, where $A$ is an $m \times n$ matrix with $m < n$, prove that if the system is consistent, then it has infinitely many solutions.
\end{problem}

\begin{problem}
Develop an algorithm to find the minimal norm solution to an underdetermined linear system $Ax = b$ (i.e., where there are more variables than equations).
\end{problem}

\begin{problem}
Consider a network with $n$ nodes. If the adjacency matrix $A$ has $(i,j)$ entry equal to 1 if there is an edge from node $i$ to node $j$ and 0 otherwise, show that the $(i,j)$ entry of $A^k$ gives the number of distinct paths of length $k$ from node $i$ to node $j$.
\end{problem}

\begin{problem}
Prove that for any symmetric matrix $A$, the system of linear equations $Ax = 0$ has a non-trivial solution if and only if $\det(A) = 0$.
\end{problem}

\section{Further Reading}

\begin{itemize}
\item Strang, G. (2016). \textit{Introduction to Linear Algebra} (5th ed.). Wellesley-Cambridge Press.
\item Axler, S. (2015). \textit{Linear Algebra Done Right} (3rd ed.). Springer.
\item Horn, R. A., \& Johnson, C. R. (2012). \textit{Matrix Analysis} (2nd ed.). Cambridge University Press.
\item Trefethen, L. N., \& Bau III, D. (1997). \textit{Numerical Linear Algebra}. SIAM.
\item Meyer, C. D. (2000). \textit{Matrix Analysis and Applied Linear Algebra}. SIAM.
\item Golub, G. H., \& Van Loan, C. F. (2013). \textit{Matrix Computations} (4th ed.). Johns Hopkins University Press.
\end{itemize}

\end{document} 